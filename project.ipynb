{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projekt\n",
    "Uruchomimy nasze środkowisko z losowymi ruchami by zobaczyć czy nasze środkowisku poprawnie się tworzy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  2  2]\n",
      " [ 4  8  4  4]\n",
      " [ 8  4 64 16]\n",
      " [ 4 16 32  4]]\n",
      "[[ 2  4  2  2]\n",
      " [ 4  8  4  4]\n",
      " [ 8  4 64 16]\n",
      " [ 4 16 32  4]]\n",
      "Max random reward: 0.04345703125\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import game2048\n",
    "\n",
    "env = gym.make('envs/game2048-v0', render_mode='human')\n",
    "\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation = env.reset(seed=42)\n",
    "\n",
    "max_random_reward = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    observation, reward, terminated, info = env.step(env.action_space.sample())\n",
    "    max_random_reward = max(max_random_reward, reward)\n",
    "    # print(f\"Reward: {info.get('reward')}\")\n",
    "    # print(f\"Board: \\n{info.get('board')}\")\n",
    "    if terminated:\n",
    "        print(info.get('board'))\n",
    "        print(info.get('board'))\n",
    "    if terminated:\n",
    "        observation = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Max random reward: {max_random_reward}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Przykładowy widok planszy w trybie wyświetlania \"human\"\n",
    "![view](project/example.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stable Baselines\n",
    "Użyjemy gotowych modeli pobranych z paczki Stable Baselines\n",
    "\n",
    "Nasza nagroda zdefiniowana jest jako suma wszystkich bloków na planszy podzielona przez 4096 by zachować ją w przedziale [0-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import game2048\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = gym.make('envs/game2048-v0')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0, device='cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pawla\\Documents\\Studia\\Python\\ml2022-23\\venv\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:190: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_env(env, warn=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100):\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    all_episode_score = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            prev_obs = obs\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                all_episode_score.append(np.max(prev_obs * (2 ** 11)))\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    mean_episode_score = np.mean(all_episode_score)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \" Mean score:\", mean_episode_score, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward, mean_episode_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 2.4911425  Mean score: 71.68 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train, mean_score_before_train = evaluate(model, num_episodes=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pawla\\Documents\\Studia\\Python\\ml2022-23\\venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:0.92 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=100_000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "env = gym.wrappers.Monitor(env, 'video/1.mp4')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    print(rewards)\n",
    "    print(obs * (2 ** 11))\n",
    "    vec_env.render(mode='human')\n",
    "while True:\n",
    "    vec_env.render(mode='human')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
